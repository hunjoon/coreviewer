{
  "confidence": 5,
  "confidence_reasoning": "You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
  "strength_and_weaknesses": "The paper has following strengths:\n\n1. I found the idea in this paper to be very interesting but I am not knowledgeable about neuroscience side of things so I cannot evaluate the accuracy of authors\u2019 design decisions from neuroscience point of view.\n\n2. The approach is well-explained, and it was still interesting to read about the relationships with biological cortex.\n\n3. Results clearly demonstrate the superiority over RepVGG and ResNets.\n\n4. Good ablations are conducted and analysis using GradCAM, etc., is conducted to understand the feature representation power of these new networks.\n\n\nThe paper has following weaknesses:\n\n1. I was hoping to see some more theoretical (or at least empirical) study on *why* the new proposed architecture would result in better representative power than traditional blocks of the same size. For example, why would the new way of grouping (e.g., after replicating the input features) should do better (in representative power) than other alternatives (e.g., standard group convolutions when scaled up to match params/flops of CoMNet units)? Since the proposed method does look similar to Group Convolutions, one important ablation should be to replace CCM with group convolution equivalents and then scaling up the model to match params/FLOPs, and then see if we get equivalent accuracy or not. Indeed, if it could be theoretically established why the new architecture would certainly do better than these alternatives, that would be ideal.\n\n2. No comparison against newer state-of-the-art methods like ConvNexts [https://arxiv.org/abs/2201.03545] or their newer scalings shown in recent works like Restructurable Activation Networks (RANs) [https://arxiv.org/abs/2208.08562] is performed which makes it unclear if these models would actually beat the newer models on ImageNet. In any case, these newer papers should be discussed in related work to reflect the latest advances in model design for image classification.\n\n3. It is quite unclear where in the model the non-linear activation functions (e.g., ReLUs) happen. Were any batchnorms/layernoms used to stabilize training? What exact activation functions were used for CoMNet and other baselines?\n\n4. Minor -- Citation format deviates from ICLR official template.\n",
  "summary_of_the_paper": "The paper presents CoMNets, a biologically inspired deep network architecture that outperforms existing methods like ResNets and RepVGG in multiple respects (e.g., Latency, FLOPs, depth, etc.). Experiments are shown on ImageNet."
}