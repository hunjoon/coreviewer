{
  "confidence": 5,
  "confidence_reasoning": "You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
  "strength_and_weaknesses": "The organization is fine. Experiments are sufficient.\n\nHowever,\n\n1.Many statements in this paper are not well motivated, e.g., ``implicit localization capability within the feature extractor\u2019\u2019 is not clear; ``exciting achievements for common categories\u2019\u2019 is also not clear. The story on comparisons among human and ZSL is commonly used in the community; you cannot say this because others have done this. The resulting facts of ZSL can not guarantee the reason of using auxiliary information for knowledge transferring. In fact, auxiliary information is one choice of conducting ZSL, there much exist other manners for performing ZSL,e.g., matching with external dataset. By saying embedding methods are inherently inferior in GZSL to generative methods, why? Also, it is fine to use unseen semantics for overcoming bias issue, yet not prone to unexpected effects. By saying unexpected effects, what do you mean? Overall, the challenges are not well defined. The writing is also not accurate.\n\n2.Terms of attribute semantics, class attribute vectors, attribute prototypes and attribute-visual features are confusing to the beginner of ZSL field. More explanation should be given. Generative adversarial network (GAN) or variational autoencoder (VAE) miss references. Typos exists: so that convert ?\n\n3.The motivation for the usage of GAT for improving the generalization is not clear. In fact, using graph neural network to model attribute relationships have been explored in previous works (e.g., LsrGAN in ECCV20). The overall compared methods are limited, and more methods should be surveyed and compared.\n\n4.The framework is actually incremental improvements on existing methods, e.g., improved on Yang et al. in CVPR 2021 by seeing each channel as attention mask. From this point, the novelty is limited to the community, since this kind of attention has been explored in the community (e.g., in AREN of CVPR2019). Furthermore, the idea of mapping AVFs into attribute semantic space by DAZLE is also not new. The idea of attribute scoring is just the combination of DAZLE with AREN for solving the same task in this paper. All these aspects reduced the contributions of this paper to the community. \n\n5.The overall writing is poor. The formulas are also confusing. Please amend them accordingly.\n\n6.How to initialize the network weights, by pre-trained ImageNet weights or from scratch? What\u2019s the specific network architecture? \n\n7.Since additional operation such as attention is used for the proposed method, I am doubted about the tradeoff between the running time and accuracy.\n\n8.The parameter analysis is shown, however, what\u2019s the specific parameters for achieving these results? E.g. \\beta. It seems the authors have reported adjusted results by varying many \\beta, this is thus not fair to compare with other methods without CS adjustments. Also, the authors used additional attribute features for model training which is also not fair compared with counterparts.",
  "summary_of_the_paper": "This paper proposes to fully utilize attributes information for GZSL for simultaneously exploring attribute alignment and enhancement (A3E). A3E consists of an attribute localization (AL) module and enhanced attribute scoring (EAS) module. AL is used for localizing attribute regions, and EAS further enhances these features by GAT. Experiments are conducted on golden GZSL datasets."
}