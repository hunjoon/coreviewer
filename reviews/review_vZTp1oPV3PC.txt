{
  "confidence": 4,
  "confidence_reasoning": "You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.",
  "strength_and_weaknesses": "\n#####Strength\n\n1. The motivation of developing a general-purpose model for molecular tasks is great and might be inspiring to the community.\n\n2. The empirical result of the proposed method is competitive, especially on PCQM4Mv2, where the task is the same as the pretraining objective.\n\n3. This manuscript is well organized and easy to follow.\n\n#####Weaknesses\n\n1. The main concern for this paper is that the novelty is not enough to me. To be specifically, both the 2D and 3D branches are proposed by previous work. The positional encodings are verified to be effective by existing work. This work combines these two branches as a single model, which is not novel in terms of technical contribution.\n\n2. Also, the empirical result on PCQM4Mv2 is good. However, the effectiveness of pretraining Transformer-M has not been demonstrated well, empirically. Specifically, the methods used for comparison in Table 2 (PDBBind) and Table 3 (QM9) do not use the same extra data for pretraining. In this sense, the comparison is not so rigorous and fair to me. Even though, the performance on QM9 is not good, except for the energy-related tasks that are consistent with the pretraining tasks. This cannot support the claim that the proposed Transformer-M is a general-purpose model.\n",
  "summary_of_the_paper": "This paper proposes Transformer-M, a Transformer-based model that can take both 2D and 3D molecular formats as input. It adopts various positional encoding techniques to unify 2D and 3D information into transformer as attention bias terms. This model is claimed to be a general-purpose model for molecular tasks. Experiments on several 2D and 3D molecular tasks have been conducted to evaluate the developed method."
}