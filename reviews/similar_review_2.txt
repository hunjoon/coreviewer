{
  "confidence": 5,
  "confidence_reasoning": "You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
  "strength_and_weaknesses": "**Strengths**\n1. The paper theoretically reveals how the noise rate affects traditional learning with noisy labels. Besides, the theoretical bounds are connected to the model capacity, which motivates the design of their framework.  \n\n2. The proposed framework is demonstrated to be effective from both the theoretical and experimental aspects. Specifically, the theoretical analyses show why and how the self-supervised features can help regularize the learning with noisy labels (weak supervision), which provides insights for understanding the benefit of self-supervised learning in making the model robust to noisy labels.\n\n**Weaknesses and Questions**\n1. The paper claims that \"early stopping will handle overfitting wrong labels at the cost of underfitting clean samples if not tuned properly\". However, the issue may be also not addressed by this paper. Perhaps, holistic regularization cannot avoid the underfitting issue well. \n\n2. It is not very new to exploit contrastive learning to handle noisy labels, e.g., [1] and [2]. More discussions about related work can be added. \n\n3. It seems that Figure 2 is not highly related to the proposed method of this paper. Besides, the power of reducing model capacities of three different paths is different. Could they always converge the same classifier?\n\n4. The analysis of the approximation error is not very solid. Besides, the trade-off analysis may contradict the double descent phenomenon in deep learning. How to address this concern?\n\n5. It is a bit hard to follow \"a larger $\\mathcal{C}$ will lead to smaller approximation error but at the cost of larger estimation error given large $N$\". It seems that the paper provides an error bound, but not the exact values. \n\n6. Assumption 2 is a bit to understand since different classes always have different memorization rates in training [3]. \n\n7. Theorem 2 is not highly related to the proposed method/framework. It seems that the theorem tells us the power of the Gaussian assumption in tackling noisy labels, which is similar to [4]. \n\n8. From Figure 5, subfigures (a)--(c) show the crossing points between fixed and unfixed encoders are different, i.e., the noise rates are 0.0, 0.4, and 0.7 for different datasets. The authors should provide some insights into this observation.\n\n9. The authors claim the performance of a fixed encoder depends on the pre-trained model. However, current experiments only focus on SimCLR. It is interesting to see whether the proposed framework works well with other self-supervised learning methods.\n\n10. It is interesting to see whether the disentangled representations would be beneficial.\n\n11. Minor comments: \n- The name of Section 3 could be revised. The memorization effect mainly claims that the deep network would first fit training data with clean labels and incorrect labels. The analysis and discussion are more related to the memorization and impacts of mislabeled data.\n- The citation format needs to be revised. Some ''\\cite{}'' need to be changed to ''\\citep{}''.  \n- It is interesting to show the effect of batch size on the effectiveness of the regularizer since the regularizer is based on self-supervised learning.\n----\n[1] Shikun Li, et al. Selective-Supervised Contrastive Learning with Noisy Labels. CVPR 2022.   \n[2] Diego Ortego, et al. Multi-objective Interpolation Training for Robustness to Label Noise. CVPR 2021.  \n[3] Yiwen Wang, et al. Symmetric Cross Entropy for Robust Learning with Noisy Labels. ICCV 2019.  \n[4] Kimin Lee, et al. Robust Inference via Generative Classifiers for Handling Noisy Labels. ICML 2019.",
  "summary_of_the_paper": "This paper discusses the memorization effect of deep neural networks and proposes a framework to mitigate the negative effects of memorizing wrong information when learning with noisy labels. The analyses focus on the tradeoff between estimation error and approximation error due to the model capacity. By two case studies, learning with a fixed encoder and learning with an unfixed encoder, the authors conclude that restricting the search space by fixing the encoder reduces the estimation error but possibly increases approximation errors. Based on the takeaways from the tradeoff analyses, the authors further propose a learning framework that tries to combine the beneficial parts of fixing the encoder (low estimation error) and not fixing the encoder (low approximation error). Specifically, they exploit the power of self-supervision that regularizes supervised learning with self-supervised features. The advantage of the proposed framework is also theoretically analyzed in a simplified case. They also run experiments on both the synthetic label noise and real-world label noise to demonstrate the effectiveness of their framework."
}