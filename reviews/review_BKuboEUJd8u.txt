{
  "confidence": 5,
  "confidence_reasoning": "You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.",
  "strength_and_weaknesses": "Strengths\n- The paper deals with an important problem of return-conditioned supervised learning methods, which is that they cannot stitch trajectories to find optimal behavior.\n- The proposed return augmentation method does seems to improve performance on various benchmarks.\n\nWeaknesses\n- The source code is not provided, which makes the credibility of the work questionable.\n- As the authors note, a major advantage of the return-conditioned SL methods is that they do not require value bootstrapping and thus does not fall into the infamous 'deadly triad'. However, adopting the return augmentation with iterative value updates can reintroduce this bootstrapping problem.\n- The return conditioning procedure is not explained in detail even though the algorithm's performance depends heavily on it (Table 2). How is the value model V_\\phi used to sample \"high, but plausible returns for the current episode\"? Also, Appendix B.2 notes that the targeted value generated from the value model is increased by some \\Delta. How is this \\Delta tuned? Overall, how much does each design choice affect the performance?\n- The performance of the proposed method is highly sensitive to N, the number of bootstrapping iterations, especially on Antmaze.\n- The D4RL benchmark results are averaged over only 3 seeds. Adding at least 1~2 more seeds would be more credible.\n\nQuestions\n- What does the numbers on Figure 3 (b) mean? Does it refer to the expected value (from V_\\phi) ?\n\n############## Post-rebuttal comment ##############\n\nI appreciate the authors for updating the manuscript, but there still are some remaining concerns.\n\nFirst, the hyperparameter tuning procedure is still somewhat ambiguous. The tuning process (search range, selection method, ...) for \\Delta is not included in the paper. Also, the authors mention that \"each method has one set of hyperparameters shared across all tasks\" on the updated Table 1, but this is wrong since the proposed method tunes \\Delta per task.\n\nSecond, I agree that the work can be interpreted as unifying value-based methods and supervised-RL, but given the empirical results, I think the proposed method fails to achieve the best of both worlds (the proposed method outperforms value-based baselines on only Antmaze). I encourage the authors to improve the method so that the cons from both approaches are minimized while the pros are maximized.",
  "summary_of_the_paper": "The paper proposes a data augmentation method for return-conditioned supervised learning that takes into account temporal compositionality. The proposed method shows higher performance compared to vanilla baselines."
}