Under review as a conference paper at ICLR 2025

BEYOND RANDOM MASKING: WHEN DROPOUT
MEETS GRAPH CONVOLUTIONAL NETWORKS

Anonymous authors
Paper under double-blind review

ABSTRACT

Graph Convolutional Networks (GCNs) have emerged as powerful tools for learn-
ing on graph-structured data, yet the behavior of dropout in these models re-
mains poorly understood. This paper presents a comprehensive theoretical anal-
ysis of dropout in GCNs, revealing its unique interactions with graph structure.
We demonstrate that dropout in GCNs creates dimension-specific stochastic sub-
graphs, leading to a form of structural regularization not present in standard neu-
ral networks. Our analysis shows that dropout effects are inherently degree-
dependent, resulting in adaptive regularization that considers the topological im-
portance of nodes. We provide new insights into dropout’s role in mitigating
oversmoothing and derive novel generalization bounds that account for graph-
specific dropout effects. Furthermore, we analyze the synergistic interaction be-
tween dropout and batch normalization in GCNs, uncovering a mechanism that
enhances overall regularization. Our theoretical findings are validated through ex-
tensive experiments on both node-level and graph-level tasks across 16 datasets.
Notably, GCN with dropout and batch normalization outperforms state-of-the-art
methods on several benchmarks. This work bridges a critical gap in the theoret-
ical understanding of regularization in GCNs and provides practical insights for
designing more effective graph learning algorithms.

1

INTRODUCTION

The remarkable success of deep neural networks across various domains has been accompanied
by the persistent challenge of overfitting, where models perform well on training data but fail to
generalize to unseen examples. This issue has spurred the development of numerous regularization
techniques, among which dropout has emerged as a particularly effective and widely adopted ap-
proach LeCun et al. (2015). Introduced by Srivastava et al. (2014), dropout addresses overfitting by
randomly ”dropping out” a proportion of neurons during training, effectively creating an ensemble
of subnetworks. This technique has proven highly successful in improving generalization and has
become a standard tool in the deep learning toolkit. The effectiveness of dropout has prompted
extensive theoretical analysis, with various perspectives offered to explain its regularization effects.

Some researchers have interpreted dropout as a form of model averaging (Baldi & Sadowski, 2013),
while others have analyzed it through the lens of information theory (Achille & Soatto, 2018). Wager
et al. (2013) provided insights into dropout’s adaptive regularization properties, and Gal & Ghahra-
mani (2016) established connections between dropout and Bayesian inference. These diverse the-
oretical frameworks have significantly enhanced our understanding of dropout’s role in mitigating
overfitting in traditional neural networks. However, as the field of deep learning has expanded to
encompass more complex data structures, particularly graphs, new questions have arisen regarding
the applicability and behavior of established techniques. Graph Neural Networks (GNNs), espe-
cially Graph Convolutional Networks (GCNs), have demonstrated remarkable performance on tasks
involving graph-structured data (Kipf & Welling, 2017). Naturally, researchers and practitioners
have applied dropout to GNNs, often observing beneficial effects on generalization (Hamilton et al.,
2017).

Despite the widespread adoption of dropout in Graph Convolutional Networks (GCNs), our prelim-
inary investigations have revealed intriguing discrepancies between its behavior in GCNs and its
well-understood effects in traditional neural networks. These observations prompt a fundamental

1

000
001

002
003
004
005
006
007

008
009
010
011
012
013

014
015
016
017
018

019
020
021
022
023
024

025
026
027
028
029
030

031
032
033
034
035
036

037
038
039
040
041

042
043
044
045
046
047

048
049
050
051
052
053

Under review as a conference paper at ICLR 2025

054
055

056
057
058
059
060
061

062
063
064
065
066
067

068
069
070
071
072

073
074
075
076
077
078

079
080
081
082
083
084

085
086
087
088
089
090

091
092
093
094
095

096
097
098
099
100
101

102
103
104
105
106
107

question: How does dropout uniquely interact with the graph structure in GCNs? In this paper, we
present a comprehensive theoretical analysis of dropout in the context of GCNs. Our findings reveal
that dropout in GCNs interacts with the underlying graph structure in ways that are fundamentally
different from its operation in traditional neural networks. Specifically, we demonstrate that:

• Dropout in GCNs creates dimension-specific stochastic sub-graphs, leading to a unique

form of structural regularization not present in standard neural networks.

• The effects of dropout are inherently degree-dependent, with differential impacts on nodes
based on their connectivity, resulting in adaptive regularization that considers the topolog-
ical importance of nodes in the graph.

• Dropout plays a crucial role in mitigating the oversmoothing problem in GCNs, though its

effects are more nuanced than previously thought.

• The generalization bounds for GCNs with dropout exhibit a complex dependence on graph

properties, diverging from traditional dropout theory.

• There exists a significant interplay between dropout and batch normalization in GCNs,

revealing synergistic effects that enhance the overall regularization.

Our theoretical framework not only provides deeper insights into the mechanics of dropout in graph-
structured data but also yields practical implications for the design and training of GCNs. We vali-
date our theoretical findings through extensive experiments on both node-level and graph-level tasks,
demonstrating the practical relevance of our analysis. This work bridges a critical gap in the theoret-
ical understanding of regularization in GCNs and paves the way for more principled approaches to
leveraging dropout in graph representation learning. Furthermore, we validate our theoretical find-
ings through extensive experiments, demonstrating that GCNs incorporating our insights on dropout
and batch normalization outperform several state-of-the-art methods on benchmark datasets, in-
cluding Cora, CiteSeer, and PubMed. This practical success underscores the importance of our
theoretical contributions and their potential to advance the field of graph representation learning.

2 RELATED WORK

Dropout in Neural Networks. Overfitting can be reduced by using dropout Hinton et al. (2012)
to prevent complex co-adaptations on the training data. Since its inception, several variants have
been proposed to enhance its effectiveness. DropConnect (Wan et al., 2013) generalizes dropout
by randomly dropping connections rather than nodes. Gaussian dropout Srivastava et al. (2014)
replaces the Bernoulli distribution with a Gaussian one for smoother regularization. Curriculum
dropout (Morerio et al., 2017) adaptively adjusts the dropout rate during training. Theoretical inter-
pretations of dropout have provided insights into its success. The model averaging perspective (Baldi
& Sadowski, 2013) views dropout as an efficient way of approximately combining exponentially
many different neural networks. The adaptive regularization interpretation (Wager et al., 2013)
shows how dropout adjusts the regularization strength for each feature based on its importance. The
Bayesian approximation view (Gal & Ghahramani, 2016) connects dropout to variational inference
in Bayesian neural networks, providing a probabilistic framework for understanding its effects.

Regularization in Graph Neural Networks. Graph Neural Networks (GNNs), while powerful,
are prone to overfitting and over-smoothing (Li et al., 2018). Various regularization techniques
have been proposed to address these issues. DropEdge (Rong et al., 2020) randomly removes edges
from the input graph during training, reducing over-smoothing and improving generalization. Graph
diffusion-based methods (Gasteiger et al., 2019) incorporate higher-order neighborhood informa-
tion to enhance model robustness. Spectral-based approaches (Wu et al., 2019) leverage the graph
spectrum to design effective regularization strategies. Empirical studies have shown that traditional
dropout can be effective in GNNs (Hamilton et al., 2017), but its interaction with graph structure re-
mains poorly understood. Some works have proposed adaptive dropout strategies for GNNs (Gao &
Ji, 2019), but these are primarily heuristic approaches without comprehensive theoretical grounding.

Theoretical Frameworks for GNNs. Despite the empirical success of Graph Neural Networks
(GNNs), establishing theories to explain their behaviors is still an evolving field. Recent works have
made significant progress in understanding over-smoothing (Li et al., 2018; Zhao & Akoglu, 2019;

2

Under review as a conference paper at ICLR 2025

Oono & Suzuki, 2019; Rong et al., 2020), interpretability (Ying et al., 2019; Luo et al., 2020; Vu
& Thai, 2020; Yuan et al., 2020; 2021), expressiveness (Xu et al., 2018; Chen et al., 2019; Maron
et al., 2018; Dehmamy et al., 2019; Feng et al., 2022), and generalization (Scarselli et al., 2018; Du
et al., 2019; Verma & Zhang, 2019; Garg et al., 2020; Zhang et al., 2020; Oono & Suzuki, 2019;
Lv, 2021; Liao et al., 2020; Esser et al., 2021; Cong et al., 2021). Our work aims to complement
these existing theoretical frameworks by focusing on the practical aspects of dropout in GNNs,
a widely used regularization technique that has not been thoroughly examined from a theoretical
perspective. Previous works have provided valuable insights using classical techniques such as
Vapnik-Chervonenkis dimension (Scarselli et al., 2018), Rademacher complexity (Lv, 2021; Garg
et al., 2020), and algorithm stability (Verma & Zhang, 2019). Recent efforts (Oono & Suzuki,
2019; Esser et al., 2021) have also made strides in incorporating the transductive learning schema of
GNNs into theoretical analyses. We bridge the gap between theoretical understanding and practical
implementation of GNNs, offering insights into how dropout affects generalization and performance
in graph-structured learning tasks.

3 THEORETICAL FRAMEWORK

In this section, we develop a rigorous mathematical framework to analyze the behavior of dropout
in Graph Convolutional Networks (GCNs). We begin by establishing notations and definitions, then
formalize the GCN model with dropout, and finally introduce key concepts that will be central to
our analysis.

3.1 NOTATIONS AND DEFINITIONS

Notations. Let G = (V, E, X) be an undirected graph with n = |V | nodes and m = |E| edges,
where X represents the node feature matrix. We denote by A ∈ Rn×n the adjacency matrix of G,
and by D = diag(d1, . . . , dn) the degree matrix, where di = (cid:80)

j Aij.

3.1.1 MATRIX DEFINITIONS AND GRAPH CONVOLUTIONAL NETWORKS (GCNS)

Definition 1 (Normalized Adjacency Matrix). The normalized adjacency matrix ˜A is defined as:

˜A = D− 1

2 AD− 1

2

(1)

Let X ∈ Rn×d0 be the input feature matrix, where d0 is the number of input features per node.
Definition 2 (L-layer GCN). An L-layer GCN is defined as a sequence of L graph convolutional
layers, where the l-th layer (l = 1, . . . , L) performs the following transformation:

H (l) = σ( ˜AH (l−1)W (l))

(2)

where H (l) ∈ Rn×dl is the feature matrix at layer l, W (l) ∈ Rdl−1×dl is the weight matrix for layer
l, σ(·) is a non-linear activation function, and H (0) = X.
Definition 3 (Feature Energy). The feature energy E(H (l)) of the node representations H (l) at
layer l is defined as:

E(H (l)) =

1
2|E|

(cid:88)

(i,j)∈E

|H (l)

i − H (l)

j

|2

3.1.2 DROPOUT IN GCNS

Definition 4 (Dropout Mask). For layer l, the dropout mask M (l) ∈ Rn×dl is a random matrix
where each element M (l)
ij

is drawn independently from a Bernoulli distribution:

M (l)

ij ∼ Bernoulli(1 − p)

(3)

where p ∈ [0, 1] is the dropout probability.

We now formally define a GCN with dropout:

3

108

109
110
111
112
113

114
115
116
117
118
119

120
121
122
123
124
125

126
127
128
129
130
131

132
133
134
135
136

137
138
139
140
141
142

143
144
145
146
147
148

149
150
151
152
153
154

155
156
157
158
159

160
161

Under review as a conference paper at ICLR 2025

Definition 5 (GCN with Dropout). For an L-layer GCN with dropout, the forward pass at layer l is
defined as:

H (l) =

M (l) ⊙ σ( ˜AH (l−1)W (l))

(4)

1
1 − p

where ⊙ denotes element-wise multiplication, and the factor
training to match the expected value at inference time.

1
1−p is used to scale the outputs during

To elucidate the specific impact of dropout on embedding features, we introduce these concepts:
Definition 6 (Dimension-specific Sub-graph). For each feature dimension j at layer l and iteration
t, we define a stochastic sub-graph G(l,j)

), where:

t = (V, E(l,j)
= {(u, v) ∈ E | M (l,t)

t

uj

E(l,j)
t

̸= 0 and M (l,t)

vj

̸= 0}

Here, M (l,t) denotes the dropout mask for layer l at iteration t.
Definition 7 (Active Path). A path P = (v0, v1, . . . , vk) in G is considered active for feature j at
layer l and iteration t if and only if:

k−1
(cid:89)

i=0

M (l,t)

vij M (l,t)

vi+1j ̸= 0.

Definition 8 (Feature-Topology Coupling Matrix). For layer l at iteration t, we define the feature-
topology coupling matrix C (l)

t ∈ Rn×n as:

t = ˜A ⊙ (M (l)
C (l)

t (M (l)

t )T )

where ˜A is the normalized adjacency matrix, M (l)
⊙ denotes the Hadamard product.

t

is the dropout mask for layer l at iteration t, and

captures how dropout affects both feature propagation and graph structure simulta-

This matrix C (l)
neously.
Definition 9 (Effective Degree). The effective degree deff

t

i (t) of node i at iteration t is defined as:

deff
i (t) =

(cid:88)

(C (l)

t )ij,

j

where C (l)

t

is the feature-topology coupling matrix defined earlier.

3.1.3

INTEGRATING BATCH NORMALIZATION (BN) AND DROPOUT IN GCNS

Definition 10 (GCN Layer with BN before activation). For a layer l, the output is defined as:

where σ is the activation function, such as ReLU, and BN is defined as:

H (l) = σ(BN( ˜AH (l−1)W (l))),

BN(X) = γ ⊙

X − µB
(cid:112)σ2
B + ϵ
B are the batch mean and variance, γ and β are learnable parameters, and ϵ is a

Here, µB and σ2
small constant for numerical stability.
Definition 11 (Feature-Topology Coupling Matrix with BN). We extend our feature-topology cou-
pling matrix C (l)

+ β.

t

to incorporate batch normalization:
t,BN = ˜A ⊙ ((M (l)
C (l)

t ⊙ BN(H (l−1)))(M (l)

t ⊙ BN(H (l−1)))T ),

where ˜A is the normalized adjacency matrix and M (l)

t

is the dropout mask for layer l at iteration t.

This formalization captures the essence of how dropout is applied in GCNs, randomly masking out
features and scaling the remaining ones. However, this seemingly simple operation interacts with
the graph structure in complex ways, which we will explore in the subsequent sections.

4

162
163

164
165
166
167
168
169

170
171
172
173
174
175

176
177
178
179
180

181
182
183
184
185
186

187
188
189
190
191
192

193
194
195
196
197
198

199
200
201
202
203

204
205
206
207
208
209

210
211
212
213
214
215

Under review as a conference paper at ICLR 2025

Figure 1: Sub-graph size.

Figure 2: Active path on Cora. Figure 3: Active path on Citeseer.

3.2 DIMENSION-SPECIFIC STOCHASTIC SUB-GRAPHS

Figure 1 shows how varying dropout rates impact the number of edges Et in stochastic sub-graphs
of a 2-layer GCN, defined by Equation 5, across the Cora and Citeseer datasets. We observe that
higher dropout rates correlate with fewer edges in these sub-graphs. This variation demonstrates
dropout’s role in GCNs as a form of structural regularization, where dimension-specific stochastic
sub-graphs are generated. Each feature dimension samples a different sub-graph from the original
graph at each iteration. This mechanism provides a rich set of structural variations during training,
potentially enhancing the model’s ability to capture diverse graph patterns.
Theorem 12 (Sub-graph Diversity). The expected number of distinct sub-graphs per iteration is:

E[|G(l,j)
t

| j = 1, . . . , dl|] = dl(1 − (1 − p)2|E|),

where dl is the number of features at layer l, p is the dropout probability, and |E| is the number of
edges in the original graph (The complete proof is in the Appendix. 12).

This theorem reveals that dropout in GCNs leads to a rich set of sub-graphs, providing a form
of structural data augmentation unique to graph-based models. The diversity of these sub-graphs
increases with both the dropout probability p and the number of features dl. This suggests that
higher-dimensional GCNs with moderate dropout rates can benefit from a wider range of struc-
tural variations during training, potentially leading to more robust and generalizable representations.
Moreover, this mechanism allows the GCN to implicitly explore different graph structures without
explicitly modifying the input graph. This could be particularly beneficial for tasks where the opti-
mal graph structure is uncertain or where multiple relevant sub-structures exist within the data.
Theorem 13 (Expected Active Features per Path). For a path P of length k, the expected number
of features for which it is active is:

E[#active features for P ] = dl(1 − p)k+1.

This theorem demonstrates that while individual long paths are unlikely to be active for any given
feature, the multi-dimensional nature of GCNs allows for effective long-range information flow
through the ensemble effect across features. Figures 2 & 3 illustrate the behavior of active features
along paths of length 1 and 2 within a 2-layer GCN equipped with 16 hidden dimensions, across
varying dropout rates. Notably, at a dropout rate of 0.6, the average number of active features
approaches zero. This characteristic also underscores the importance of multidimensional feature
spaces in ensuring robust information transmission under feature dropout.

3.3 DEGREE-DEPENDENT NATURE OF DROPOUT EFFECTS

The interaction between dropout and the graph structure leads to a form of degree-dependent regu-
larization in GCNs. This means that the effect of dropout varies based on the connectivity of each
node, creating an adaptive regularization scheme that considers the topological importance of nodes
in the graph.
Theorem 14 (Degree-Dependent Dropout Effect). The expected effective degree and its variance
are given by:

E[deff

i (t)] = (1 − p)2di and Var[deff

i (t)] = di(1 − p)2(1 − (1 − p)2),

(5)

where di is the original degree of node i and p is the dropout probability).

5

216
217

218
219
220
221
222
223

224
225
226
227
228
229

230
231
232
233
234

235
236
237
238
239
240

241
242
243
244
245
246

247
248
249
250
251
252

253
254
255
256
257

258
259
260
261
262
263

264
265
266
267
268
269

Under review as a conference paper at ICLR 2025

Figure 4: Effective degree.

Figure 5: Effective CV vs degree. Figure 6: Accuracy on Cora.

This theorem highlights that dropout affects nodes differentially depending on their degree. High-
degree nodes, typically more influential within the graph, exhibit less variation in their effective
degree due to dropout, potentially resulting in more stable representations for these important nodes.
This observation is empirically confirmed in the analysis of a 2-layer GCN presented in Figure 6.
Consequently, the degree-dependent nature of dropout in GCNs results in adaptive regularization,
where the regularization effect naturally adjusts to the local graph structure.

Corollary 15 (Relative Stability of High-Degree Nodes). The coefficient of variation of the effective
degree, defined as CV [deff

i (t)], decreases with increasing node degree:

i (t)]/E[deff

i (t)] =

Var[deff

(cid:113)

CV [deff

i (t)] =

(cid:112)1 − (1 − p)2
di(1 − p)

√

.

This corollary further confirms that high-degree nodes experience relatively less variation in their
effective degree due to dropout. Figure 5 illustrates that the CV decreases as node degree increases.
This degree-dependent effect distinguishes dropout in GCNs from its application in standard neural
networks and suggests that the optimal dropout strategy for GCNs may need to consider the graph
structure explicitly.

3.4 ROLE OF DROPOUT IN OVERSMOOTHING

Oversmoothing is a well-known issue in GCNs, where node representations become indistinguish-
able as the number of layers increases. Our analysis reveals that dropout plays a crucial role in
this context, though its effects are more nuanced than previously thought. While dropout introduces
noise that can help preserve feature diversity, it may also contribute to feature smoothing in certain
conditions.

Theorem 16 (Dropout and Feature Energy). For a GCN with dropout probability p, the expected
feature energy at layer l is bounded by:

E[E(H (l))] ≥ (1 − p)lE(X) −

2p
1 − p

l−1
(cid:88)

(1 − p)kTr(W (l−k)(W (l−k))T ),

k=0

where E(X) is the energy of the input features and W (k) are the weight matrices (The complete
proof is in the Appendix.16).

This theorem provides critical insights into how dropout influences oversmoothing in GCNs. The
first term, (1 − p)lE(X), highlights a depth-dependent smoothing effect, indicating that dropout can
lead to an exponential decrease in feature energy as network depth increases. This suggests that in
very deep networks, dropout alone might not adequately prevent oversmoothing. The second term
underscores the diversity introduced by dropout, acting as noise injection that helps maintain feature
diversity, particularly in the earlier layers. Furthermore, the trace term involving the weight matrices
reveals that the magnitude of the weights significantly impacts how dropout affects feature energy,
pointing to a potential interaction between dropout and weight regularization techniques.

Corollary 17 (Dropout and Oversmoothing). In deep GCNs with many layers, dropout may accel-
erate oversmoothing, with the effect becoming more pronounced as the dropout rate p increases.

6

270
271

272
273
274
275
276
277

278
279
280
281
282
283

284
285
286
287
288

289
290
291
292
293
294

295
296
297
298
299
300

301
302
303
304
305
306

307
308
309
310
311

312
313
314
315
316
317

318
319
320
321
322
323

Under review as a conference paper at ICLR 2025

324

325
326
327
328
329

330
331
332
333
334
335

336
337
338
339
340
341

342
343
344
345
346
347

348
349
350
351
352

353
354
355
356
357
358

359
360
361
362
363
364

365
366
367
368
369
370

371
372
373
374
375

376
377

Figure 7: Feature energy vs dropout rates.

Figure 8: BN feature energy vs dropout rates.

This analysis reveals a paradox: while dropout preserves the expected label homophily of the graph,
it may accelerate feature oversmoothing. This suggests that the relationship between homophily,
oversmoothing, and model performance is more complex than previously thought.

These findings challenge the simple view of dropout as a universal solution to oversmoothing in
GCNs. Instead, they suggest that effective strategies to combat oversmoothing may need to consider
the interplay between dropout, network depth, and other regularization techniques.

3.5 GENERALIZATION BOUNDS WITH GRAPH-SPECIFIC DROPOUT EFFECTS

The unique properties of dropout in GCNs, such as the creation of stochastic sub-graphs and degree-
dependent effects, influence how these models generalize to unseen data. Our analysis provides
novel generalization bounds that explicitly account for these graph-specific dropout effects, offer-
ing insights into how dropout interacts with graph structure to influence the model’s generalization
capabilities.
Theorem 18 (Generalization Bound for L-Layer GCN with Dropout). For an L-layer GCN F with
dropout, with probability at least 1 − δ over the training examples, the following generalization
bound holds:

ED[L(F (x))] − ES[L(F (x))] ≤ O





(cid:115)

(λmax( ˆA))L · ((cid:80)L

l=1
n

pl
1−pl

Rl) · log(1/δ)



 ,

(6)

where ED is the expectation over the data distribution, ES is the expectation over the train-
ing samples, L is the loss function, λmax( ˜A) is the largest eigenvalue of the normalized ad-
jacency matrix, n is the number of training samples, and Rl are stability terms defined as:
Rl = ES[|Jl(x)T H(x)Jl(x)|F | ˜Ahl−1|2

F ] (The complete proof is in the Appendix.18).

This generalization bound provides a theoretical framework for understanding how dropout in GCNs
balances between reducing overfitting and maintaining the model’s ability to capture complex graph
patterns. It suggests that optimal dropout strategies for GCNs may need to be graph-aware, poten-
tially adapting the dropout rate based on graph properties and network depth.

3.6

INTERACTION OF DROPOUT AND BATCH NORMALIZATION IN GCNS

While dropout provides a powerful regularization mechanism for GCNs, its degree-dependent nature
can lead to uneven regularization across nodes. Batch Normalization (BN) offers a complementary
approach that can potentially address this issue and enhance the benefits of dropout. Our analysis
reveals how the combination of dropout and BN creates a synergistic regularization effect that is
sensitive to both graph structure and feature distributions.
Theorem 19 (Feature-Topology Decoupling Effect of BN). The expected feature-topology coupling
matrix with batch normalization before ReLU is:

E[C (l)

t,BN] = (1 − p)2 ˜A ⊙ E[BN(H (l−1))(BN(H (l−1)))T ],

where ˜A is the normalized adjacency matrix, p is the dropout probability, and BN(H (l−1)) is the
batch-normalized output of the previous layer (The complete proof is in the Appendix.19).

7

Under review as a conference paper at ICLR 2025

This theorem suggests that applying BN before dropout in GCNs can enhance the feature-topology
decoupling effect, resulting in reduced structural dependency by altering the correlation structure of
features, thereby lessening the influence of the graph structure on feature representations.
Theorem 20 (Batch Normalization and Feature Energy). For a GCN with dropout probability p and
batch normalization before ReLU, the expected feature energy at layer l is lower bounded by:

· E(X)−

γ2
σ2
B + ϵ
γ2
k
σ2
B,k + ϵ

E[E(H (l))] ≥

(1 − p)l
2|E|

· E[σ′(BN( ˜AH (l−1)W (l)))] ·

p
|E|(1 − p)

l−1
(cid:88)

k=0

(1 − p)k · E[σ′(BN( ˜AH (k−1)W (k)))] ·

· Tr(W (l−k)(W (l−k))T ),

where E(X) is the energy of the input features, γ and σ2
and ϵ is a small constant for numerical stability (The complete proof is in the Appendix.20.

B are the batch normalization parameters,

This theorem sheds light on the interaction between BN and dropout within GCNs, revealing several
critical dynamics. Firstly, the inclusion of BN parameters (γ and σ2
B) in the analysis suggests that
BN can help maintain feature diversity across network layers. This is particularly valuable in com-
bating the potential oversmoothing effects of dropout, especially in deeper networks. Additionally,
the role of E[σ′(BN(·))] terms emphasizes the significant interplay between BN, dropout, and non-
linear activation functions. This synergy is crucial for preserving feature diversity and addressing
potential gradient issues, enhancing the overall robustness and effectiveness of deep GCNs.

The synergistic effect of dropout and BN in GCNs, as demonstrated by the referenced theorems,
provides a refined form of regularization. Figures 7 & 8 illustrate how BN mitigates the issue of
excessive energy levels caused by dropout. These findings highlight that the integration of dropout
and BN in GCNs forms a unique mechanism that effectively addresses the complexities inherent
in learning from graph-structured data. This suggests that future GCN architectures could benefit
significantly from regularization strategies that are specifically tailored to exploit this interplay.

4 EXPERIMENTS

To validate our theoretical analysis, we conducted extensive experiments on a variety of datasets,
considering both node-level and graph-level tasks. We implemented our dropout technique on
several popular GNN architectures: GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al.,
2017), GAT (Veliˇckovi´c et al., 2018), and GatedGCN (Bresson & Laurent, 2017). For each
model, we compared the performance with and without dropout. Our code is available at https:
//anonymous.4open.science/r/dropout-theory.

4.1 DATASETS AND SETUP

Datasets. For node-level tasks, we used 12 datasets: Cora, CiteSeer, PubMed (Sen et al., 2008),
ogbn-arxiv, ogbn-products (Hu et al., 2020), Amazon-Computer, Amazon-Photo, Coauthor-CS,
Coauthor-Physics (Shchur et al., 2018), WikiCS (Mernyei & Cangea, 2020), Amazon-Ratings,
Roman-Empire, Minesweeper, and Questions (Platonov et al., 2023). For graph-level tasks, we
used MNIST, CIFAR10 (Dwivedi et al., 2023), and two Peptides datasets (functional and structural)
(Hu et al., 2020).

Baselines. Our main focus lies on the following prevalent GNNs and transformer models from
Polynormer (Deng et al., 2024): GCN (Kipf & Welling, 2017), SAGE (Hamilton et al., 2017), GAT
Veliˇckovi´c et al. (2018), GCNII (Chen et al., 2020), (Veliˇckovi´c et al., 2018), APPNP (Gasteiger
et al., 2018), GPRGNN (Chien et al., 2020), SGFormer (Wu et al., 2023), Polynormer (Deng et al.,
2024), GOAT (Kong et al., 2023), NodeFormer (Wu et al., 2022), NAGphormer (Chen et al., 2022),
GTDwivedi & Bresson (2020), SAN Kreuzer et al. (2021), MGT Ngo et al. (2023), DRew Gutteridge
et al. (2023), Graph-MLPMixer He et al. (2023), GRIT Ma et al. (2023) , GraphGPS (Ramp´aˇsek
et al., 2022), Exphormer (Shirzad et al., 2023), CKGCN (Ma et al., 2024), GRED (Ding et al.,
2024), Graph Mamba Behrouz & Hashemi (2024). We report the performance results of baselines
primarily from (Deng et al., 2024), with the remaining obtained from their respective original papers
or official leaderboards whenever possible, as those results are obtained by well-tuned models.

8

378
379

380
381
382
383
384
385

386
387
388
389
390
391

392
393
394
395
396

397
398
399
400
401
402

403
404
405
406
407
408

409
410
411
412
413
414

415
416
417
418
419

420
421
422
423
424
425

426
427
428
429
430
431

Under review as a conference paper at ICLR 2025

Table 1: Node classification results (%). The baseline results are taken from Deng et al. (2024); Wu
et al. (2023). The top 1st, 2nd and 3rd results are highlighted. ”dp” denotes dropout.

Cora

CiteSeer

PubMed Computer

Photo

CS

Physics WikiCS

ogbn-arxiv ogbn-products

# nodes
# edges
Metric

GCNII
GPRGNN
APPNP
tGNN

GraphGPS
NAGphormer
Exphormer
GOAT
NodeFormer
SGFormer
Polynormer

GCN
Dirichlet energy

GCN w/o dp
Dirichlet energy

2,708
5,278

2,449,029
61,859,140
Accuracy↑ Accuracy↑ Accuracy↑ Accuracy↑ Accuracy↑ Accuracy↑ Accuracy↑ Accuracy↑ Accuracy↑ Accuracy↑

169,343
1,166,243

13,752
245,861

7,650
119,081

34,493
247,962

11,701
216,123

19,717
44,324

18,333
81,894

3,327
4,732

85.19 ± 0.26 73.20 ± 0.83 80.32 ± 0.44 91.04 ± 0.41 94.30 ± 0.20 92.22 ± 0.14 95.97 ± 0.11 78.68 ± 0.55 72.74 ± 0.31
83.17 ± 0.78 71.86 ± 0.67 79.75 ± 0.38 89.32 ± 0.29 94.49 ± 0.14 95.13 ± 0.09 96.85 ± 0.08 78.12 ± 0.23 71.10 ± 0.12
83.32 ± 0.55 71.78 ± 0.46 80.14 ± 0.22 90.18 ± 0.17 94.32 ± 0.14 94.49 ± 0.07 96.54 ± 0.07 78.87 ± 0.11 72.34 ± 0.24
82.97 ± 0.68 71.74 ± 0.49 80.67 ± 0.34 83.40 ± 1.33 89.92 ± 0.72 92.85 ± 0.48 96.24 ± 0.24 71.49 ± 1.05 72.88 ± 0.26

82.84 ± 1.03 72.73 ± 1.23 79.94 ± 0.26 91.19 ± 0.54 95.06 ± 0.13 93.93 ± 0.12 97.12 ± 0.19 78.66 ± 0.49 70.97 ± 0.41
82.12 ± 1.18 71.47 ± 1.30 79.73 ± 0.28 91.22 ± 0.14 95.49 ± 0.11 95.75 ± 0.09 97.34 ± 0.03 77.16 ± 0.72 70.13 ± 0.55
82.77 ± 1.38 71.63 ± 1.19 79.46 ± 0.35 91.47 ± 0.17 95.35 ± 0.22 94.93 ± 0.01 96.89 ± 0.09 78.54 ± 0.49 72.44 ± 0.28
83.18 ± 1.27 71.99 ± 1.26 79.13 ± 0.38 90.96 ± 0.90 92.96 ± 1.48 94.21 ± 0.38 96.24 ± 0.24 77.00 ± 0.77 72.41 ± 0.40
82.20 ± 0.90 72.50 ± 1.10 79.90 ± 1.00 86.98 ± 0.62 93.46 ± 0.35 95.64 ± 0.22 96.45 ± 0.28 74.73 ± 0.94 59.90 ± 0.42
84.50 ± 0.80 72.60 ± 0.20 80.30 ± 0.60 92.42 ± 0.66 95.58 ± 0.36 95.71 ± 0.24 96.75 ± 0.26 80.05 ± 0.46 72.63 ± 0.13
83.25 ± 0.93 72.31 ± 0.78 79.24 ± 0.43 93.68 ± 0.21 96.46 ± 0.26 95.53 ± 0.16 97.27 ± 0.08 80.10 ± 0.67 73.46 ± 0.16

85.22 ± 0.66 73.24 ± 0.63 81.08 ± 1.16 93.15 ± 0.34 95.03 ± 0.24 94.41 ± 0.13 97.07 ± 0.04 80.14 ± 0.52 73.13 ± 0.27
3.765

735.876

20.241

0.452

7.403

8.020

0.437

8.021

8.966

83.18 ± 1.22 70.48 ± 0.45 79.40 ± 1.02 90.60 ± 0.84 94.10 ± 0.15 94.30 ± 0.22 96.92 ± 0.05 77.61 ± 1.34 72.05 ± 0.23
1.793

264.230

0.114

0.592

2.951

0.170

1.231

3.980

0.318

79.42 ± 0.36
79.76 ± 0.59
78.84 ± 0.09
81.79 ± 0.54

OOM
73.55 ± 0.21
OOM
82.00 ± 0.43
73.96 ± 0.30
81.54 ± 0.43
83.82 ± 0.11

81.87 ± 0.41
7.771

77.50 ± 0.37
1.745

GCN w/o BN

84.97 ± 0.73 72.97 ± 0.86 80.94 ± 0.87 92.39 ± 0.18 94.38 ± 0.13 93.46 ± 0.24 96.76 ± 0.06 79.00 ± 0.48 71.93 ± 0.18

79.37 ± 0.42

84.14 ± 0.63 71.62 ± 0.29 77.86 ± 0.79 92.65 ± 0.21 95.71 ± 0.20 95.90 ± 0.09 97.20 ± 0.10 80.29 ± 0.97 72.72 ± 0.13
SAGE
SAGE w/o dp
83.06 ± 0.80 69.68 ± 0.82 76.40 ± 1.48 90.17 ± 0.60 94.90 ± 0.17 95.80 ± 0.08 97.06 ± 0.06 78.84 ± 1.17 71.37 ± 0.31
SAGE w/o BN 83.89 ± 0.67 71.39 ± 0.75 77.26 ± 1.02 92.54 ± 0.24 95.51 ± 0.23 94.87 ± 0.15 97.03 ± 0.03 79.50 ± 0.93 71.52 ± 0.17

GAT
GAT w/o dp
GAT w/o BN

83.92 ± 1.29 72.00 ± 0.91 80.48 ± 0.99 93.47 ± 0.27 95.53 ± 0.16 94.49 ± 0.17 96.73 ± 0.10 80.21 ± 0.68 72.83 ± 0.19
82.58 ± 1.47 71.08 ± 0.42 79.28 ± 0.58 92.94 ± 0.30 93.88 ± 0.16 94.30 ± 0.14 96.42 ± 0.08 78.67 ± 0.40 71.52 ± 0.41
83.76 ± 1.32 71.82 ± 0.83 80.43 ± 1.03 92.16 ± 0.26 95.05 ± 0.49 93.33 ± 0.26 96.57 ± 0.20 79.49 ± 0.62 71.68 ± 0.36

82.69 ± 0.28
79.82 ± 0.22
80.91 ± 0.35

80.05 ± 0.34
77.87 ± 0.25
78.21 ± 0.32

Experimental Setup. We implemented all models using the PyTorch Geometric library (Fey &
Lenssen, 2019). The experiments are conducted on a single workstation with 8 RTX 3090 GPUs.
For node-level tasks, we adhered to the training protocols specified in (Deng et al., 2024), employing
BN and adjusting the dropout rate between 0.1 and 0.7.
In graph-level tasks, we followed the
experimental settings established by T¨onshoff et al. (2023), utilizing BN with a consistent dropout
rate of 0.2. All experiments were run with 5 different random seeds, and we report the mean accuracy
and standard deviation. To ensure generalizability, we used Dirichlet energy (Cai & Wang, 2020) as
an oversmoothing metric, which is proportional to our feature energy (see appendix).

4.2 NODE-LEVEL CLASSIFICATION RESULTS

The node-level classification results in Table 1 not only align with our theoretical predictions but also
showcase the remarkable effectiveness of our approach. Notably, GCN with dropout and batch nor-
malization outperforms state-of-the-art methods on several benchmarks, including Cora, CiteSeer,
and PubMed. This superior performance underscores the practical significance of our theoretical
insights. Consistently across all datasets, models employing dropout outperform their counterparts
without it, validating our analysis that dropout provides beneficial regularization in GNNs, distinct
from its effects in standard neural networks. The varying levels of improvement observed across
different datasets support our theory of degree-dependent dropout effects that adapt to the graph
structure. Furthermore, the consistent increase in Dirichlet energy when using dropout provides em-
pirical evidence for our theoretical insight into dropout’s crucial role in mitigating oversmoothing in
GCNs, particularly evident in larger graphs. The complementary roles of dropout and batch normal-
ization are demonstrated by the performance drop when either is removed, supporting our analysis
of their synergistic interaction in GCNs.

4.3 GRAPH-LEVEL CLASSIFICATION RESULTS

Our graph-level classification results, presented in Tables 2 and 3, further validate the broad applica-
bility of our theoretical framework. The significant improvements in accuracy when using dropout
on graph-level tasks, such as Peptides and CIFAR10, demonstrate that our insights extend beyond
node classification. The consistent reduction in Dirichlet energy when using dropout supports our
theoretical analysis of dropout’s role in preserving feature diversity, even as it may accelerate certain
aspects of oversmoothing. The varying degrees of improvement across different graph datasets align
with our theory that dropout provides adaptive regularization based on graph properties.

9

432
433

434
435
436
437
438
439

440
441
442
443
444
445

446
447
448
449
450

451
452
453
454
455
456

457
458
459
460
461
462

463
464
465
466
467
468

469
470
471
472
473

474
475
476
477
478
479

480
481
482
483
484
485

Under review as a conference paper at ICLR 2025

486
487

488
489
490
491
492
493

494
495
496
497
498
499

500
501
502
503
504

505
506
507
508
509
510

511
512
513
514
515
516

517
518
519
520
521
522

523
524
525
526
527

528
529
530
531
532
533

534
535
536
537
538
539

Table 2: Graph classification results on two pep-
tide datasets from LRGB (Dwivedi et al., 2022).

Table 3: Graph classification results on two im-
age datasets from (Dwivedi et al., 2023).

Model

# graphs
Avg. # nodes
Avg. # edges
Metric

GT
SAN+RWSE
GraphGPS
MGT+WavePE
DRew
Exphormer
Graph-MLPMixer
GRIT
CKGCN
GRED
Graph Mamba

GCN w/o dp
Dirichlet energy

GCN
Dirichlet energy

Peptides-func

Peptides-struct

15,535
150.9
307.3
AP ↑

0.6326 ± 0.0126
0.6439 ± 0.0075
0.6535 ± 0.0041
0.6817 ± 0.0064
0.7150 ± 0.0044
0.6527 ± 0.0043
0.6970 ± 0.0080
0.6988 ± 0.0082
0.6952 ± 0.0068
0.7085 ± 0.0027
0.6972 ± 0.0100

0.6484 ± 0.0034
6.817

0.7015 ± 0.0021
2.535

15,535
150.9
307.3
MAE ↓

0.2529 ± 0.0016
0.2545 ± 0.0012
0.2500 ± 0.0012
0.2453 ± 0.0025
0.2536 ± 0.0015
0.2481 ± 0.0007
0.2475 ± 0.0015
0.2460 ± 0.0012
0.2477 ± 0.0019
0.2503 ± 0.0019
0.2477 ± 0.0019

0.2541 ± 0.0026
3.769

0.2437 ± 0.0012
1.492

Model

# graphs
Avg. # nodes
Avg. # edges
Metric

GT
SAN+RWSE
GraphGPS
MGT+WavePE
DRew
Exphormer
Graph-MLPMixer
GRIT
CKGCN
GRED
Graph Mamba

MNIST

CIFAR10

70,000
70.6
564.5
Accuracy ↑

90.831 ± 0.161
-
98.051 ± 0.126
-
-
98.550 ± 0.039
97.422 ± 0.110
98.108 ± 0.111
98.423 ± 0.155
98.383 ± 0.012
98.392 ± 0.183

60,000
117.6
941.1
Accuracy ↑

59.753 ± 0.293
-
72.298 ± 0.356
-
-
74.696 ± 0.125
73.961 ± 0.330
76.468 ± 0.881
72.785 ± 0.436
76.853 ± 0.185
74.563 ± 0.379

GatedGCN w/o dp
Dirichlet energy

98.235 ± 0.136
25.852

71.384 ± 0.397
20.671

GatedGCN
Dirichlet energy

98.783 ± 0.122
10.593

78.231 ± 0.274
11.026

These experimental results provide compelling empirical support for our theoretical analysis,
demonstrating that dropout in GCNs indeed creates dimension-specific stochastic sub-graphs, ex-
hibits degree-dependent effects, plays a nuanced role in oversmoothing, and provides adaptive reg-
ularization that considers graph topology. The synergistic effects with batch normalization and the
generalization to graph-level tasks underscore the broad applicability and importance of our the-
oretical framework. Most importantly, the superior performance of GCN with dropout and batch
normalization across various benchmarks highlights the practical impact of our theoretical insights,
paving the way for more effective and theoretically grounded GNN architectures.

5 CONCLUSIONS

Our comprehensive theoretical analysis of dropout in Graph Convolutional Networks (GCNs) has
unveiled complex interactions between regularization, graph structure, and model performance that
challenge traditional understanding. These insights not only deepen our understanding of how
dropout functions in graph-structured data but also open new avenues for research and develop-
ment in graph representation learning. Our findings suggest the need to reimagine regularization
techniques for graph-based models, explore adaptive and structure-aware dropout strategies, and
carefully balance local and global information in GCN architectures. Furthermore, the observed
synergies between dropout and batch normalization point towards more holistic approaches to reg-
ularization in GNNs. As we move forward, this work lays a foundation for developing more robust
and effective graph learning algorithms, with potential applications in dynamic graphs, large-scale
graph sampling, and adversarial robustness. Ultimately, this research contributes to bridging the
gap between the empirical success of GNNs and their theoretical foundations, paving the way for
designing graph learning models.

REFERENCES

Alessandro Achille and Stefano Soatto.

Information dropout: Learning optimal representations
through noisy computation. IEEE transactions on pattern analysis and machine intelligence, 40
(12):2897–2905, 2018.

Pierre Baldi and Peter J Sadowski. Understanding dropout. Advances in neural information pro-

cessing systems, 26, 2013.

Ali Behrouz and Farnoosh Hashemi. Graph mamba: Towards learning on graphs with state space
In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and

models.
Data Mining, pp. 119–130, 2024.

10

Under review as a conference paper at ICLR 2025

540
541

542
543
544
545
546
547

548
549
550
551
552
553

554
555
556
557
558

559
560
561
562
563
564

565
566
567
568
569
570

571
572
573
574
575
576

577
578
579
580
581

582
583
584
585
586
587

588
589
590
591
592
593

Xavier Bresson and Thomas Laurent.

Residual gated graph convnets.

arXiv preprint

arXiv:1711.07553, 2017.

Chen Cai and Yusu Wang. A note on over-smoothing for graph neural networks. arXiv preprint

arXiv:2006.13318, 2020.

Jinsong Chen, Kaiyuan Gao, Gaichao Li, and Kun He. Nagphormer: A tokenized graph transformer
for node classification in large graphs. In The Eleventh International Conference on Learning
Representations, 2022.

Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
convolutional networks. In International conference on machine learning, pp. 1725–1735. PMLR,
2020.

Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with gnns. Advances in neural information
processing systems, 32, 2019.

Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank

graph neural network. In International Conference on Learning Representations, 2020.

Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On provable benefits of depth in training
graph convolutional networks. Advances in Neural Information Processing Systems, 34:9936–
9949, 2021.

Nima Dehmamy, Albert-L´aszl´o Barab´asi, and Rose Yu. Understanding the representation power of
graph neural networks in learning graph topology. Advances in Neural Information Processing
Systems, 32, 2019.

Chenhui Deng, Zichao Yue, and Zhiru Zhang. Polynormer: Polynomial-expressive graph trans-

former in linear time. arXiv preprint arXiv:2403.01232, 2024.

Yuhui Ding, Antonio Orvieto, Bobby He, and Thomas Hofmann. Recurrent distance filtering for
In Forty-first International Conference on Machine Learning,

graph representation learning.
2024.

Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. Advances in
neural information processing systems, 32, 2019.

Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs.

arXiv preprint arXiv:2012.09699, 2020.

Vijay Prakash Dwivedi, Ladislav Ramp´aˇsek, Mikhail Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu,
and Dominique Beaini. Long range graph benchmark. arXiv preprint arXiv:2206.08164, 2022.

Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and
Xavier Bresson. Benchmarking graph neural networks. Journal of Machine Learning Research,
24(43):1–48, 2023.

Pascal Esser, Leena Chennuru Vankadara, and Debarghya Ghoshdastidar. Learning theory can
(sometimes) explain generalisation in graph neural networks. Advances in Neural Information
Processing Systems, 34:27043–27056, 2021.

Jiarui Feng, Yixin Chen, Fuhai Li, Anindya Sarkar, and Muhan Zhang. How powerful are k-hop
message passing graph neural networks. Advances in Neural Information Processing Systems, 35:
4776–4790, 2022.

Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric.

arXiv preprint arXiv:1903.02428, 2019.

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050–1059.
PMLR, 2016.

11

Under review as a conference paper at ICLR 2025

Hongyang Gao and Shuiwang Ji. Graph u-nets. In international conference on machine learning,

pp. 2083–2092. PMLR, 2019.

Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits
In International Conference on Machine Learning, pp. 3419–3430.

of graph neural networks.
PMLR, 2020.

Johannes Gasteiger, Aleksandar Bojchevski, and Stephan G¨unnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018.

Johannes Gasteiger, Stefan Weißenberger, and Stephan G¨unnemann. Diffusion improves graph

learning. Advances in neural information processing systems, 32, 2019.

Benjamin Gutteridge, Xiaowen Dong, Michael M Bronstein, and Francesco Di Giovanni. Drew: Dy-
namically rewired message passing with delay. In International Conference on Machine Learning,
pp. 12252–12267. PMLR, 2023.

Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.

Advances in neural information processing systems, 30, 2017.

Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson. A
generalization of vit/mlp-mixer to graphs. In International Conference on Machine Learning, pp.
12724–12745. PMLR, 2023.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov.
Improving neural networks by preventing co-adaptation of feature detectors. arxiv 2012. arXiv
preprint arXiv:1207.0580, 2012.

Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances
in neural information processing systems, 33:22118–22133, 2020.

Thomas N. Kipf and Max Welling.

Semi-supervised classification with graph convolutional
In International Conference on Learning Representations, 2017. URL https:

networks.
//openreview.net/forum?id=SJU4ayYgl.

Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Renkun Ni, C. Bayan Bruss, and Tom Gold-
stein. GOAT: A global transformer on large-scale graphs.
In Andreas Krause, Emma Brun-
skill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Pro-
ceedings of the 40th International Conference on Machine Learning, volume 202 of Proceed-
ings of Machine Learning Research, pp. 17375–17390. PMLR, 23–29 Jul 2023. URL https:
//proceedings.mlr.press/v202/kong23a.html.

Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L´etourneau, and Prudencio Tossou. Re-
thinking graph transformers with spectral attention. Advances in Neural Information Processing
Systems, 34:21618–21629, 2021.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,

2015.

Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks
for semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence,
volume 32, 2018.

Renjie Liao, Raquel Urtasun, and Richard Zemel. A pac-bayesian approach to generalization bounds

for graph neural networks. arXiv preprint arXiv:2012.07690, 2020.

Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang
Zhang. Parameterized explainer for graph neural network. Advances in neural information pro-
cessing systems, 33:19620–19631, 2020.

Shaogao Lv. Generalization bounds for graph convolutional neural networks via rademacher com-

plexity. arXiv preprint arXiv:2102.10234, 2021.

12

594
595

596
597
598
599
600
601

602
603
604
605
606
607

608
609
610
611
612

613
614
615
616
617
618

619
620
621
622
623
624

625
626
627
628
629
630

631
632
633
634
635

636
637
638
639
640
641

642
643
644
645
646
647

Under review as a conference paper at ICLR 2025

648
649

650
651
652
653
654
655

656
657
658
659
660
661

662
663
664
665
666

667
668
669
670
671
672

673
674
675
676
677
678

679
680
681
682
683
684

685
686
687
688
689

690
691
692
693
694
695

696
697
698
699
700
701

Liheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, Puneet K Dokania, Mark Coates,
Philip Torr, and Ser-Nam Lim. Graph inductive biases in transformers without message passing.
arXiv preprint arXiv:2305.17589, 2023.

Liheng Ma, Soumyasundar Pal, Yitian Zhang, Jiaming Zhou, Yingxue Zhang, and Mark Coates.
Ckgconv: General graph convolution with continuous kernels. arXiv preprint arXiv:2404.13604,
2024.

Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph

networks. arXiv preprint arXiv:1812.09902, 2018.

P´eter Mernyei and C˘at˘alina Cangea. Wiki-cs: A wikipedia-based benchmark for graph neural net-

works. arXiv preprint arXiv:2007.02901, 2020.

Pietro Morerio, Jacopo Cavazza, Riccardo Volpi, Ren´e Vidal, and Vittorio Murino. Curriculum
dropout. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3544–
3552, 2017.

Nhat Khang Ngo, Truong Son Hy, and Risi Kondor. Multiresolution graph transformers and wavelet
positional encoding for learning long-range and hierarchical structures. The Journal of Chemical
Physics, 159(3), 2023.

Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node

classification. arXiv preprint arXiv:1905.10947, 2019.

Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova.
A critical look at the evaluation of gnns under heterophily: Are we really making progress? arXiv
preprint arXiv:2302.11640, 2023.

Ladislav Ramp´aˇsek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Do-
minique Beaini. Recipe for a general, powerful, scalable graph transformer. arXiv preprint
arXiv:2205.12454, 2022.

Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classification. In International Conference on Learning Repre-
sentations, 2020. URL https://openreview.net/forum?id=Hkx1qkrKPr.

Franco Scarselli, Ah Chung Tsoi, and Markus Hagenbuchner. The vapnik–chervonenkis dimension

of graph and recursive neural networks. Neural Networks, 108:248–259, 2018.

Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.

Collective classification in network data. AI magazine, 29(3):93–93, 2008.

Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G¨unnemann. Pitfalls

of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.

Hamed Shirzad, Ameya Velingker, Balaji Venkatachalam, Danica J Sutherland, and Ali Kemal
Sinop. Exphormer: Sparse transformers for graphs. arXiv preprint arXiv:2303.06147, 2023.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929–1958, 2014.

Jan T¨onshoff, Martin Ritzert, Eran Rosenbluth, and Martin Grohe. Where did the gap go? reassess-

ing the long-range graph benchmark. arXiv preprint arXiv:2309.00367, 2023.

Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.

Saurabh Verma and Zhi-Li Zhang. Stability and generalization of graph convolutional neural net-
works. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Dis-
covery & Data Mining, pp. 1539–1548, 2019.

13

Under review as a conference paper at ICLR 2025

702
703

704
705
706
707
708
709

710
711
712
713
714
715

716
717
718
719
720

721
722
723
724
725
726

727
728
729
730
731
732

733
734
735
736
737
738

739
740
741
742
743

744
745
746
747
748
749

750
751
752
753
754
755

Minh Vu and My T Thai. Pgm-explainer: Probabilistic graphical model explanations for graph
neural networks. Advances in neural information processing systems, 33:12225–12235, 2020.

Stefan Wager, Sida Wang, and Percy S Liang. Dropout training as adaptive regularization. Advances

in neural information processing systems, 26, 2013.

Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural
networks using dropconnect. In International conference on machine learning, pp. 1058–1066.
PMLR, 2013.

Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Sim-
In International conference on machine learning, pp.

plifying graph convolutional networks.
6861–6871. PMLR, 2019.

Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. Nodeformer: A scalable graph
structure learning transformer for node classification. Advances in Neural Information Processing
Systems, 35:27387–27401, 2022.

Qitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui Zhang, Fan Nie, Haitian Jiang, Yatao Bian,
and Junchi Yan. Simplifying and empowering transformers for large-graph representations. In
Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://
openreview.net/forum?id=R4xpvDTWkV.

Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural

networks? In International Conference on Learning Representations, 2018.

Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer:
Generating explanations for graph neural networks. Advances in neural information processing
systems, 32, 2019.

Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. Xgnn: Towards model-level explanations of
graph neural networks. In Proceedings of the 26th ACM SIGKDD international conference on
knowledge discovery & data mining, pp. 430–438, 2020.

Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural
networks via subgraph explorations. In International conference on machine learning, pp. 12241–
12252. PMLR, 2021.

Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong. Fast learning of graph neural
networks with guaranteed generalizability: one-hidden-layer case. In International Conference
on Machine Learning, pp. 11268–11277. PMLR, 2020.

Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. arXiv preprint

arXiv:1909.12223, 2019.

A APPENDIX

A.1 PROOF OF THEOREM 12

Proof. Let’s approach this proof:

1) For a single feature j, the probability that an edge is present in the sub-graph G(l,j)
as both endpoints need to retain this feature.

t

is (1 − p)2,

2) The probability that an edge is not present in G(l,j)

t

is 1 − (1 − p)2 = p(2 − p).

3) For a sub-graph to be identical to the original graph, all edges must be present. The probability
of this is: ((1 − p)2)|E| = (1 − p)2|E|.

4) Therefore, the probability that G(l,j)
p)2|E|.

t

is different from the original graph (i.e., unique) is 1 − (1 −

14

Under review as a conference paper at ICLR 2025

5) Define an indicator random variable Xj for each feature j:

Xj =

(cid:40)

1
0

if G(l,j)
t
otherwise

is unique

.

6) We have:

P (Xj = 1) = 1 − (1 − p)2|E|][P (Xj = 0) = (1 − p)2|E|.

7) The expected value of Xj is:

E[Xj] = 1 · P (Xj = 1) + 0 · P (Xj = 0) = 1 − (1 − p)2|E|.

8) The total number of unique sub-graphs is (cid:80)dl

j=1 Xj. By the linearity of expectation:

E[|G(l,j)
t

| j = 1, . . . , dl|] = E[

dl(cid:88)

j=1

Xj] =

dl(cid:88)

j=1

E[Xj] = dl(1 − (1 − p)2|E|).

This completes the proof.

A.2 PROOF OF THEOREM 16

Proof. We start with the definition of feature energy:

Taking the expectation:

E(H (l)) =

E[E(H (l))] =

1
2|E|

(cid:88)

i,j

1
2|E|

(cid:88)

i,j

|h(l)

i − h(l)
j |2

E[|h(l)

i − h(l)

j |2]

Now, let’s focus on E[|h(l)

E[|h(l)

i − h(l)

j |2] =

=

=

=

=

j |2]:

E[|M (l)

i ⊙ zi|2 + |M (l)

j ⊙ zj|2 − 2(M (l)

i − h(l)
1
(1 − p)2
1
(1 − p)2 ((1 − p)|zi|2 + (1 − p)|zj|2 − 2(1 − p)2zT
1
1 − p
1
1 − p
1
1 − p

(|zi|2 + |zj|2 − 2(1 − p)zT

(|zi|2 + |zj|2) − 2zT

(|zi − zj|2 + p(zT

i zi + zT

j zj))

i zj)

i zj

i zj)

i ⊙ zi)T (M (l)

j ⊙ zj)]

where zi = σ((cid:80)

k

˜Aikh(l−1)

k W (l)). Using the properties of the dropout mask:

E[|h(l)

i − h(l)

j |2] =

=

1
1 − p
1
1 − p

(|zi|2 + |zj|2) − 2zT

i zj

(|zi − zj|2 + p(|zi|2 + |zj|2))

Now, using the Lipschitz continuity of σ (assume Lipschitz constant 1 for ReLU):

|zi − zj|2 ≥ |

(cid:88)

k

( ˜Aik − ˜Ajk)h(l−1)

k W (l)|2

And bounding the squared norms:

|zi|2 + |zj|2 ≤ 2|W (l)|2
F

15

756
757

758
759
760
761
762
763

764
765
766
767
768
769

770
771
772
773
774

775
776
777
778
779
780

781
782
783
784
785
786

787
788
789
790
791
792

793
794
795
796
797

798
799
800
801
802
803

804
805
806
807
808
809

Under review as a conference paper at ICLR 2025

Substituting these bounds:

E[|h(l)

i − h(l)

j |2] ≥ |

(cid:88)

( ˜Aik − ˜Ajk)h(l−1)

k W (l)|2 −

k

2p
1 − p

|W (l)|2
F

Applying this recursively and using the energy of the input features E(X), we get:

E[E(H (l))] ≥ (1 − p)lE(X) −

2p
1 − p

l−1
(cid:88)

(1 − p)kTr(W (l−k)(W (l−k))T )

k=0

A.3

PROOF OF THEOREM 18

Proof. The proof proceeds in several steps:

Step 1: Dropout Effect as Perturbation. Consider the effect of dropout at layer l. We can model
the effect of dropout as a perturbation δ(l) to the layer output:

δ(l) =

1
1 − p

M (l) ⊙ σ( ˜AH (l−1)W (l)) − σ( ˜AH (l−1)W (l)),

(7)

where M (l) is the dropout mask with elements drawn from Bernoulli(1-p). This formulation directly
represents the difference between the output with and without dropout.

Step 2: Perturbation Propagation. We analyze how the perturbation at each layer affects the final
output. Let Fl(x) be the output of the network when dropout is applied up to layer l. We can bound
the difference between Fl(x) and Fl−1(x):

∥Fl(x) − Fl−1(x)∥ ≤ ∥Jl(x)∥ · ∥δ(l)∥,

(8)

where Jl(x) is the Jacobian of F with respect to the layer output.

Step 3: Bounding Perturbation Magnitude. We can bound the expected squared magnitude of
δ(l):

E[∥δ(l)∥2] = E[∥

M (l) ⊙ σ( ˜AH (l−1)W (l)) − σ( ˜AH (l−1)W (l))∥2]

1
1 − p
p
(1 − p)

=

∥σ( ˜AH (l−1)W (l))∥2
F .

This is because E[(M (l))2] = E[M (l)] = 1 − p.

Step 4: Loss Stability. Using the Taylor expansion of the loss function and the perturbation bounds,
we can bound the expected change in loss due to dropout at layer l:

(cid:34)
E[|L(Fl(x)) − L(Fl−1(x))|] ≤ E

|∇L(F (x))T (Fl(x) − Fl−1(x))|

(9)

(10)

(11)

(12)

(cid:35)
∥Fl(x) − Fl−1(x)∥T H(x)∥Fl(x) − Fl−1(x)∥

E[∥Jl(x)T H(x)Jl(x)∥F · ∥σ( ˜AH (l−1)W (l))∥2

F ].

(13)

+

1
2
p
(1 − p)

≤

Step 5: Aggregating Across Layers. The total expected change in loss due to dropout across all
layers can be bounded by summing the individual layer effects:

E[|L(F (x)) − L(Fno dropout(x))|] ≤

L
(cid:88)

Rl,

(14)

where Rl = p

(1−p)

l=1
ES[∥Jl(x)T H(x)Jl(x)∥F · ∥σ( ˜AH (l−1)W (l))∥2

F ].

16

810
811

812
813
814
815
816
817

818
819
820
821
822
823

824
825
826
827
828

829
830
831
832
833
834

835
836
837
838
839
840

841
842
843
844
845
846

847
848
849
850
851

852
853
854
855
856
857

858
859
860
861
862
863

Under review as a conference paper at ICLR 2025

Step 6: Graph Structure Effect. Each application of ˜A can amplify perturbations by at most
λmax( ˜A). Over L layers, this leads to the (λmax( ˜A))L term in the bound.
Step 7: Generalization Bound. We now apply McDiarmid’s inequality. Let S = {x1, ..., xn} be
the training set. Changing one example in S can change the learned model, and thus the loss on any
point, by at most 2(λmax( ˜A))L (cid:80)L

l=1 Rl/n. Applying McDiarmid’s inequality:
(cid:32)

2nϵ2

P (ED[L(F (x))] − ES[L(F (x))] > ϵ) ≤ exp

−

(cid:33)

.

(15)

4(λmax( ˜A))2L((cid:80)L

l=1 Rl)2

Setting the right-hand side to δ and solving for ϵ gives the bound in the theorem statement, up to
constant factors.

A.4 PROOF OF THEOREM 19

Proof. 1) First, recall the definition of the feature-topology coupling matrix with BN:

t,BN = ˜A ⊙ ((M (l)
C (l)

t ⊙ BN(H (l−1)))(M (l)

t ⊙ BN(H (l−1)))T ).

2) Let’s focus on the expectation of the term (M (l)

E[(M (l)

t ⊙ BN(H (l−1)))(M (l)

t ⊙ BN(H (l−1)))(M (l)
t ⊙ BN(H (l−1)))T ].

t ⊙ BN(H (l−1)))T :

3) We can rewrite this as:

E[(M (l)

t ⊙ BN(H (l−1)))(M (l)
t = diag(M (l)

where D(l)

t ⊙ BN(H (l−1)))T ] = E[D(l)

t BN(H (l−1))(BN(H (l−1)))T (D(l)

t )T ],

t ) is a diagonal matrix with the elements of M (l)

t on its diagonal.

4) Now, we can use the linearity of expectation and the fact that M (l)

t

is independent of BN(H (l−1)):

E[D(l)

t BN(H (l−1))(BN(H (l−1)))T (D(l)

t )T ] = E[D(l)

t

]E[BN(H (l−1))(BN(H (l−1)))T ]E[(D(l)

t )T ].

5) We know that each element of M (l)
Therefore:

t

is 1 with probability (1 − p) and 0 with probability p.

E[D(l)
t

] = E[(D(l)

t )T ] = (1 − p)I,

where I is the identity matrix.

6) Substituting this back:

E[D(l)

t BN(H (l−1))(BN(H (l−1)))T (D(l)

t )T ] = (1 − p)2E[BN(H (l−1))(BN(H (l−1)))T ].

7) Now, let’s consider the full expression for C (l)

t,BN:

E[C (l)

t,BN] = E[ ˜A ⊙ ((M (l)

t ⊙ BN(H (l−1)))(M (l)

t ⊙ BN(H (l−1)))T )].

8) Since ˜A is constant with respect to the expectation, we can move it outside:

E[C (l)

t,BN] = ˜A ⊙ E[(M (l)

t ⊙ BN(H (l−1)))(M (l)

t ⊙ BN(H (l−1)))T ].

9) Substituting the result from step 6:

E[C (l)

t,BN] = ˜A ⊙ ((1 − p)2E[BN(H (l−1))(BN(H (l−1)))T ]).

Finally, we can rearrange this to get our desired result:

E[C (l)

t,BN] = (1 − p)2 ˜A ⊙ E[BN(H (l−1))(BN(H (l−1)))T ]

This completes the proof.

17

864
865

866
867
868
869
870
871

872
873
874
875
876
877

878
879
880
881
882

883
884
885
886
887
888

889
890
891
892
893
894

895
896
897
898
899
900

901
902
903
904
905

906
907
908
909
910
911

912
913
914
915
916
917

Under review as a conference paper at ICLR 2025

A.5 PROOF OF THEOREM 20

Proof. We start with the modified definition of feature energy:

Taking the expectation:

E(H (l)) =

1
2|E|

(cid:88)

(i,j)∈E

E[E(H (l))] =

1
2|E|

(cid:88)

(i,j)∈E

The GCN layer definition remains the same:

|h(l)

i − h(l)

j |2.

E[|h(l)

i − h(l)

j |2].

h(l)

i =

1
1 − p

M (l)

i ⊙ σ(BN(

(cid:88)

˜Aikh(l−1)

k W (l))).

k

Let zi = BN((cid:80)

˜Aikh(l−1)

k W (l)). Then:

k

E[|h(l)

i − h(l)

j |2] =

1
1 − p

(|σ(zi)|2 + |σ(zj)|2) − 2E[σ(zi)T σ(zj)].

Using the mean value theorem approximation:

E[|h(l)

i − h(l)

j |2] ≥ E[σ′(ξ)]|zi − zj|2 −

2p
1 − p

|W (l)|2
F .

The batch normalization step remains the same:

zi = γ

(cid:80)
k

˜Aikh(l−1)
k W (l) − µB
(cid:112)σ2

B + ϵ

+ β.

This gives us:

γ2
σ2
B + ϵ
Substituting back and applying recursively:

|zi − zj|2 =

|

(cid:88)

( ˜Aik − ˜Ajk)h(l−1)

k W (l)|2.

k

E[|h(l)

i − h(l)

j |2] ≥(1 − p)l · E[σ′(BN( ˜AH (l−1)W (l)))] ·

γ2
σ2
B + ϵ

· |h(0)

i − h(0)

j

|2−

2p
1 − p

l−1
(cid:88)

k=0

(1 − p)k · E[σ′(BN( ˜AH (k−1)W (k)))] ·

γ2
k
σ2
B,k + ϵ

· |W (l−k)|2
F .

Taking the average over all edges:

E[E(H (l))] ≥

(1 − p)l
2|E|

· E[σ′(BN( ˜AH (l−1)W (l)))] ·

p
|E|(1 − p)

l−1
(cid:88)

k=0

(1 − p)k · E[σ′(BN( ˜AH (k−1)W (k)))] ·

· Tr(W (l−k)(W (l−k))T ).

· E(X)−

γ2
σ2
B + ϵ
γ2
k
σ2
B,k + ϵ

This completes the proof.

18

918
919

920
921
922
923
924
925

926
927
928
929
930
931

932
933
934
935
936

937
938
939
940
941
942

943
944
945
946
947
948

949
950
951
952
953
954

955
956
957
958
959

960
961
962
963
964
965

966
967
968
969
970
971

